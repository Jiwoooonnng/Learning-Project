{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11 Policy Gradients and Optimization\n",
    "\n",
    "## 1. Policy Gradients\n",
    "\n",
    "- 정책을 파라미터 $\\theta$로 근사하여 직접 최적화하는 방법\n",
    "\n",
    "$$ \\pi(a|s;\\theta)\\; \\leftarrow\\;  \\pi(a|s) $$\n",
    "\n",
    "- 장점\n",
    "> - 연속적인 행동 공간을 다룰 수 있는 장점이 있음\n",
    "> - 무한대의 행동과 상태를 갖는 경우에도 적용 가능  \n",
    "> $\\rightarrow$ 환경으로부터 받는 보상이 최대가 되도록 파라미터를 직접 업데이트\n",
    "\n",
    "- 정책 신경망\n",
    "> - 최적 정책을 정의하기 위한 신경망\n",
    "> - 입력 : 상태, 출력 : 입력 상태에서 각 행동의 확률\n",
    ">\n",
    "> - 행동과 함께 보상을 함께 저장\n",
    "> - 높은 보상 $\\rightarrow$ 높은 확률, 낮은 보상 $\\rightarrow$ 낮은 확률\n",
    "\n",
    "\n",
    "## 2. Deep Deterministic Policy Gradients(DDPG)\n",
    "\n",
    "- Actor-Critic Architecture\n",
    "> - PG와 State-Value Function을 조합한 방법\n",
    "> - Actor : 최적의 행동을 결정, 최적의 파라미터를 튜닝하여 찾아야 함\n",
    "> - Critic : Actor의 행동을 평가, 행동을 평가할 떄 TD에러를 이용\n",
    ">  \n",
    "> - 상태를 정의하는 특징 값의 같은 분산, 평균을 적용하기 위해 Batch Normalization을 진행\n",
    "> - Orenstein-Uhlenbeck random process : 연속적인 환경을 탐험하기 위해 Actor에서 생성된 환경에 행동 잡은 N을 추가\n",
    "\n",
    "- DDPG 알고리즘\n",
    "> ### Actor Network\n",
    "> - $\\mu(s:\\theta^{u})$ : Actor Network\n",
    "> - $\\theta^{u}$ : Actor Parameter\n",
    "> - $\\mu(s:\\theta^{u'})$ : target of Actor Network  \n",
    ">  \n",
    "> - 입력 : 상태\n",
    "> - 출력 : 주어진 상태에서의 행동\n",
    "> \n",
    "> - 학습 : PG 방법으로 업데이트\n",
    ">\n",
    "> ### Critic Network\n",
    "> - $Q(s,a:\\theta^{Q})$ : Critic Network\n",
    "> - $\\theta^{Q}$ : Critic Parameter\n",
    "> - $Q(s,a:\\theta^{Q'})$ : target of Critic Network\n",
    "> - 타겟 큐 값\n",
    ">  \n",
    "> $$ y_{i} = r_{i} + \\gamma Q'(s_{i+1}, \\mu'(s_{i+1}|\\theta^{u'})|\\theta^{Q'}) $$   \n",
    ">  \n",
    "> - TD 에러(M은 리플레이 버퍼에서 샘플링한 샘플의 개수)\n",
    "> \n",
    "> $$ L \\; = \\; \\frac{1}{M} \\sum(y_{i} - Q(s_{i},a_{i}|\\theta^{Q}))^{2} $$\n",
    ">\n",
    "> - 학습 : TD에러로 계산된 Gradient Descent\n",
    ">\n",
    "> ### 매개변수 업데이트\n",
    "> - 소프트 타겟 업데이트\n",
    "> $$ \\theta^{Q'} \\; \\leftarrow \\; \\tau \\theta^{Q} + (1 - \\tau)\\theta^{Q'} $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
