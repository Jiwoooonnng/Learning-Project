{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3. MDP and DP\n",
    "## 1. Markov Chain and Markov Process\n",
    "\n",
    "- Markov Property : 미래 상태가 현재 상태에만 의존\n",
    "- Markov Chain : 오로지 현재 상태에만 의존하여 다음 상태를 예측하는 확률 모델\n",
    "\n",
    "> ### 용어\n",
    "> - Transition : 한 상태에서 다른 상태로 옮겨가는 것\n",
    "> - Transition Probability : 다른 상태로 옮겨갈 확률\n",
    "\n",
    "## 2. MDP(Markov Decision Process)\n",
    "\n",
    "> ### A. Value and Return\n",
    "> - Value(r) : 수행한 행동의 보상\n",
    "> - Return(R) : 환경으로 받느 미래 보상의 총합  \n",
    "> $$ R_{t} = r_{t+1} + r_{t+2} + ... + r_{T} $$\n",
    "\n",
    "> ### B. Episodic and Nonepisodic Task\n",
    "> - Episodic : 최종 상태가 있는 작업\n",
    "> - Nonepisodic : 최종 상택 없는 작업\n",
    "\n",
    "> ### C. Discount Factor\n",
    "> - 이유 : Continuous 작업에서는 $R_{t}$이 $ \\infty $가 될 수 있음\n",
    "> - 해결  \n",
    "> $$ R_{t} = \\sum_{k}^{\\infty} \\gamma ^{k}r_{t+k+1} $$\n",
    "\n",
    "> ### D. Policy Function($\\pi$)\n",
    "> - $\\pi_{s} : s \\rightarrow a $  \n",
    "> - 목표 : Optimal Policy를 찾는 것\n",
    "\n",
    "> ### E. Stable-Value Function(=Value Function)\n",
    "> - $\\pi$로 행동하는 에이전트가 특정 상태에 있는 것이 얼마나 가치 있는지 알려줌  \n",
    "> $$ V^{\\pi}{(s)} = E_{\\pi}[R_{t}|s_{t} = s] = E_{\\pi}[\\sum_{k}^{\\infty} \\gamma ^{k}r_{t+k+1}|s_{t} = s]$$\n",
    "\n",
    "> ### G. State-Action Function(=Q Function)\n",
    "> - $\\pi$로 행동할 때, 각 상태에서 특정 행동을 하는 것이 얼마나 가치 있는지 나타내는 함수\n",
    "> $$ Q^{\\pi}{(s,a)} = E_{\\pi}[R_{t}|s_{t} = s, a_{t} = a] $$\n",
    "\n",
    ">> ### 용어\n",
    ">> - State($S$) : 에이전트가 놓일 수 있는 상태($s$)의 집합\n",
    ">> - Action($A$) : 에이전트가 한 상태에서 다른 상태로 옮겨가기 위해 행할 수 있는 행동($a$)의 집합  \n",
    ">> - Transition Probability($P_{ss^{'}}^{a}$) : $a : s \\rightarrow s^{'}$로 옮겨가는 전이확률  \n",
    ">> - Reward Probability($R_{ss^{'}}^{a}$) : $a : s \\rightarrow s^{'}$로 옮겨가며 받는 보상확률\n",
    ">> - Discount Factor($\\gamma$) : 현재와 미래 보상의 중요도제어\n",
    "\n",
    "## 3. Bellman Equation\n",
    "\n",
    "- MDP의 해를 찾을 수 있음\n",
    "- MDP의 해 : 최적 정책과 최적 가치 함수를 찾는 것\n",
    "\n",
    "- 최적 가치 함수 : 항상 큰 가치를 갖는 가치함수 = 큐함수의 최댓값\n",
    "\n",
    "$$ V^{*}(s) = \\max_{\\pi} V^{*}(s) = \\max_{a} Q^{\\pi}(s,a) $$  \n",
    "\n",
    "- 가치함수에 대한 벨만 방정식\n",
    "\n",
    "$$ V^{\\pi}(s) = \\sum_{a} \\pi(s,a) \\sum_{s^{'}} P_{ss^{'}}^{a}[R_{ss^{'}}^{a} + \\gamma V^{\\pi}(s^{'})] $$\n",
    "\n",
    "- 큐함수에 대한 벨만 방정식\n",
    "\n",
    "$$ Q^{\\pi}(s,a) = \\sum_{s^{'}} P_{ss^{'}}^{a}[R_{ss^{'}}^{a} + \\gamma Q^{\\pi}(s^{'},a^{'})] $$\n",
    "\n",
    "- 다시 표현한 최적 가치 함수의 벨만 최적 방정식\n",
    "\n",
    "$$ V^{*}(s) = \\max_{a} \\sum_{s^{'}} P_{ss^{'}}^{a}[R_{ss^{'}}^{a} + \\gamma Q^{\\pi}(s^{'},a^{'})] $$\n",
    "\n",
    "\n",
    "## 4. Solution of Bellman Equation\n",
    "\n",
    "> ### Dynamic Programming\n",
    "> - 문제를 여러개의 하위 문제로 나누고, 각 하위 문제의 해를 저장하며 이를 재활용하는 방식\n",
    ">\n",
    ">> #### Value Iteration\n",
    ">> - 랜덤 가치함수로 시작\n",
    ">> - 여러번의 반복을 통해 최적 가치 함수 찾아감\n",
    ">> \n",
    ">> <img src=\"Theory%20images/VI.png\" /> \n",
    ">\n",
    ">> #### Policy Iteration\n",
    ">> - 랜덤 정책으로 시작\n",
    ">> - 여러번의 반복을 통해 최적 가치 함수 찾아감\n",
    ">>\n",
    ">> 1. Policy Evaluation : 현재 정책의 가치함수 평가\n",
    ">> 2. Policy Improvement : 평가된 가치함수가 최적이 아니면 새 정책으로 갱신\n",
    ">> \n",
    ">> <img src=\"Theory%20images/PI.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
