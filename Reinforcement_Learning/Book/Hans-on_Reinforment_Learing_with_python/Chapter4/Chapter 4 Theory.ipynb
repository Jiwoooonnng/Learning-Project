{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chaper 4 Monte Carlo Methods\n",
    "\n",
    "## 1. Monte Carlo Methods\n",
    "\n",
    "* 랜덤 샘플링을 통해 근사된 해답을 구하는 통계 기법\n",
    "* 여러 번 시도하여 나온 결과의 확률을 근사\n",
    "\n",
    "## 2. Prediction with Monte Carlo Methods\n",
    "\n",
    "* Model Free 학습 알고리즘(모델의 동적 특성을 모를 경우 사용)\n",
    "* 상태, 행동, 보상으로 구성된 샘플만 필요\n",
    "\n",
    "* Episodic 작업에만 사용 가능\n",
    "\n",
    "> ### First Visit Monte Carlo\n",
    "> * 특정 상태를 처음 방문한 경우에만 총 누적 보상의 기댓값(or 평균값)을 계산\n",
    "\n",
    "> ### Every Visit Monte Carlo\n",
    "> * 한 에피소드에서 특정 상태를 방문할 때 마다 총 누적 보상의 기댓값(or 평균값)을 계산\n",
    "\n",
    "## 3. Control of Monte Carlo Methods\n",
    "\n",
    "* 가치함수를 최적화하는 방법\n",
    "* 제어에는 Generalized Policy Iteration(GPI)가 사용됨(Policy Imporvement와 Policy Evaluation의 상호 작용)\n",
    "\n",
    "* Monte Carlo는 상태 가치를 추정하지 않고 \\bf('행동 가치')를 구함\n",
    "* 최적 행동을 알기 위해, 각 상태에서 가능한 모든 행동을 탐험해야 함\n",
    "\n",
    "> ### Monte Carlo Exploring Starts(MC-ES)\n",
    "> * 각 에피소드마다 임의의 상태에서 시작하여 탐험적으로 행동을 수행\n",
    "> * 많은 수의 에피소드가 있을 경우, 가능한 모든 행동으로 모든 상태를 포괄\n",
    "> \n",
    "> <img src=\"theory images/MC_ES.png\">\n",
    ">\n",
    "> Monte Carlo 탐험 개시는 엄청난 개수의 상태와 행동이 존재하면 많은 시간이 걸림\n",
    ">> #### A. On-Policy Monte Carlo Control\n",
    ">> - Epsilon-Greedy 정책을 사용\n",
    ">> - Greedy : 순간의 가장 좋은 결정을 선택\n",
    ">> - Epsilon의 확률로 무작위 행동을 탐험(Exploring) 1-Epsilon 확률로 Greedy한 선택(Exploiting)\n",
    ">\n",
    ">> #### B. Off-Policy Monte Carlo Control\n",
    ">> - Behavior Policy와 Target Policy 2가지 정책을 사용\n",
    ">> - Behavior Policy : 에이전트가 따르는 정책, Exploring(=Soft) 정책\n",
    ">> - Target Policy : 에이전트를 평가하고 개선하는 정책, Exploiting 정책\n",
    ">>\n",
    ">> - 목표 : Target Policy $\\pi$에 대한 $Q(s,a)$를 추정\n",
    ">> \n",
    ">>> ##### Importance Sampling 기법 : Behavior Policy와 Target Policy 사이 공통 에피소드 추정\n",
    ">>> - 다른 샘플로 부터 주어진 한 분포에서 가치 추정  \n",
    ">>> - a. 일반 중요도 샘플링 : Behavior Policy와 Target Policy의 총누적보상의 비율  \n",
    ">>> - b. 가중치가 적용된 중요도 샘플링 : Behavior Policy와 Target Policy의 가중 평균\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
