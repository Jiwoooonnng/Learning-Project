{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chaper 4 Monte Carlo Methods\n",
    "\n",
    "## 1. Monte Carlo Methods\n",
    "\n",
    "* 랜덤 샘플링을 통해 근사된 해답을 구하는 통계 기법\n",
    "* 여러 번 시도하여 나온 결과의 확률을 근사\n",
    "\n",
    "## 2. Prediction with Monte Carlo Methods\n",
    "\n",
    "* Model Free 학습 알고리즘(모델의 동적 특성을 모를 경우 사용)\n",
    "* 상태, 행동, 보상으로 구성된 샘플만 필요\n",
    "\n",
    "* Episodic 작업에만 사용 가능\n",
    "\n",
    "> ### First Visit Monte Carlo\n",
    "> * 특정 상태를 처음 방문한 경우에만 총 누적 보상의 기댓값(or 평균값)을 계산\n",
    "\n",
    "> ### Every Visit Monte Carlo\n",
    "> * 한 에피소드에서 특정 상태를 방문할 때 마다 총 누적 보상의 기댓값(or 평균값)을 계산\n",
    "\n",
    "## 3. Control of Monte Carlo Methods\n",
    "\n",
    "* 가치함수를 최적화하는 방법\n",
    "* 제어에는 Generalized Policy Iteration(GPI)가 사용됨(Policy Imporvement와 Policy Evaluation의 상호 작용)\n",
    "\n",
    "* Monte Carlo는 상태 가치를 추정하지 않고 \\bf('행동 가치')를 구함\n",
    "* 최적 행동을 알기 위해, 각 상태에서 가능한 모든 행동을 탐험해야 함\n",
    "\n",
    "> ### Monte Carlo Exploring Starts(MC-ES)\n",
    "> * 각 에피소드마다 임의의 상태에서 시작하여 탐험적으로 행동을 수행\n",
    "> * 많은 수의 에피소드가 있을 경우, 가능한 모든 행동으로 모든 상태를 포괄\n",
    "> \n",
    "> <img src=\"theory images/MC_ES.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
