{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8 Atari Games with Deep Q Network\n",
    "\n",
    "## Deep Q Network(DQN)\n",
    "\n",
    "- Q-Learning에서 업데이트 식\n",
    "\n",
    "$$ Q(s,a) \\; = \\; Q(s,a) \\;+\\; \\alpha(r \\;+\\; \\gamma \\max Q(s^{'}, a) \\;-\\; Q(s,a)) $$\n",
    "\n",
    "- 모든 상태와 행동에 대해 시도할 수 없는 환경에서 최적의 큐함수\n",
    "\n",
    "$$ Q^{*}(s,a) \\leftarrow Q(s,a;\\theta) $$\n",
    "\n",
    "- $\\theta$를 매개변수로 하는 신경망을 활용하여 Q-Learning\n",
    "- DQN에서 손실함수 업데이트\n",
    "\n",
    "$$ Loss = (y_{i} - Q(s,a;\\theta))^{2} $$\n",
    "\n",
    "$$ y_{i} = r + \\gamma \\max_{a'}Q(s^{'},a^{'};\\theta) $$\n",
    "\n",
    "\n",
    "## DQN Architecture\n",
    "\n",
    "### Convolution Network\n",
    "> - CONV => CONV, ReLu => Fully Connected Layer\n",
    ">> ### 입력\n",
    ">> - 게임 화면을 입력\n",
    ">> - 팩맨이 움직이는 방향을 알기 위해 이전 4개의 게임 화면을 포함한 총 5개의 게임 화면 사용\n",
    "\n",
    ">> ### 출력\n",
    ">> - 출력은 상태의 행동 개수와 같게 설정\n",
    ">> - 특정상태에서 할 수 있는 모든 행동에 대한 큐 값을 얻음\n",
    "\n",
    "### Experience Replay\n",
    "> - 전이정보 <s,a,r,s'>이 리플레이 버퍼에 저장됨\n",
    "> - 리플레이 버퍼에서 임의로 선택한 전이로 DQN 학습\n",
    "> - 경험간의 상관관계가 줄어 다양한 경험을 통해 더 잘 학습할 수 있음\n",
    "\n",
    "###  Target Network\n",
    "> - 타겟값, 예측값 둘 다 동일한 파라미터 $\\theta$를 사용하기 때문에 발산이 생길 수 있음\n",
    "> - 이 문제를 피하기 위해 Target Network 구성\n",
    "> \n",
    "> - 손실함수 : 타겟 큐 값($Q(s',a;\\theta')$), 예측 큐 값($Q(s,a;\\theta)$)의 차이의 제곱\n",
    "> \n",
    "> $$ Loss = (r + \\gamma \\max_{a'}Q(s',a;\\theta') - Q(s,a;\\theta))^{2} $$\n",
    "> \n",
    "\n",
    "### 보상 범위 제한\n",
    "> - DQN은 모든 보상을 -1과 +1로 만듦\n",
    "\n",
    "### DQN알고리즘 순서\n",
    "> 1. 게임상태 전처리\n",
    "> 2. DQN이 해당상태에서 가능한 모든 상태의 큐 값 반환\n",
    "> 3. Epsilon-Greedy 정책에 따라 행동을 선택\n",
    "> 4. 선택된 행동에 따라 상태를 이동하며 보상을 받음\n",
    "> 5. 전이를 리플레이 버퍼에 저장\n",
    "> 6. 리플레이 버퍼에서 임의의 전이 배치를 선택하여 손실 값을 계산\n",
    "> 7. 손실함수 계산\n",
    "> 8. 손실함수를 최소화 하기 위한 최적화(경사 하강법)\n",
    "> 9. 매 단계마다 네트워크의 파라미터 $\\theta$를 타겟 파라미터 $\\theta'$에 복사\n",
    "> 10. 에피소드 마다 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
